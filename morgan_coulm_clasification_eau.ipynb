{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eda:\n",
    "    -importation et afichage des 3 df\n",
    "    -tentative de concatenation des 3 df\n",
    "    -reparation des featur de l'annees 2019 et re-concatenation\n",
    "    -traitement des type de donnés\n",
    "    -traitement des données manquantes\n",
    "    -Créer un dictionnaire de mappage pour renommer les valeurs\n",
    "    -changer les valeurs objet en format numerique\n",
    "    -recuperation des 10 colone qui on le plus de corelation avec la target \"Classification\"\n",
    "ia stuff\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>eda</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importation et afichage des 3 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df18 = pd.read_csv('./data/data-2018-64df4976a53ca451602301.csv')\n",
    "df19 = pd.read_csv('./data/data-2019-64df49821d485822963415.csv')\n",
    "df20 = pd.read_csv('./data/data-2020-64df49899653e310510744.csv')\n",
    "print('df 2018')\n",
    "df18.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df 2019')\n",
    "df19.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df 2020')\n",
    "df20.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tentative de concatenation des 3 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df18, df19, df20], ignore_index=True)\n",
    "df_concat.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "heatmap = sns.heatmap(df_concat.isna(), cmap='coolwarm')\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reparation des featur de l'annees 2019 et re-concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df20 = df20.drop('Unnamed: 8', axis=1)\n",
    "print(df18.columns.to_list())\n",
    "print(df19.columns.to_list())\n",
    "print(df20.columns.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df19 = df19.rename(columns={'EC': 'E.C','CO_-2 ': 'CO3','HCO_ - ': 'HCO3','Cl -': 'Cl','F -': 'F','NO3- ': 'NO3 ','SO4-2': 'SO4', 'Na+': 'Na', 'K+': 'K', 'Ca+2': 'Ca', 'Mg+2': 'Mg'})\n",
    "df_concat = pd.concat([df18, df19, df20], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df18.columns.to_list())\n",
    "print(df19.columns.to_list())\n",
    "print(df20.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "heatmap = sns.heatmap(df_concat.isna(), cmap='coolwarm')\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traitement des type de donnés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat['pH'] = df_concat['pH'].replace('8..05', '8.05')\n",
    "df_concat['pH'] = df_concat['pH'].astype(float)\n",
    "df_concat['pH'].dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traitement des données manquantes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colonne CO3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pourcentage de donné manquante sur les 3 ans',df_concat['CO3'].isnull().sum() / len(df_concat['CO3']))\n",
    "print('pourcentage de donné manquante en 2019',df19['CO3'].isnull().sum() / len(df19['CO3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.drop('CO3', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colonne gwl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat['gwl'].value_counts()\n",
    "df_concat['gwl'].fillna(df_concat['gwl'].mean(), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "afichage des données manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "heatmap = sns.heatmap(df_concat.isna(), cmap='coolwarm')\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un dictionnaire de mappage pour renommer les valeurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = {\n",
    "    'C2S1': 'Moderate',\n",
    "    'C3S1': 'Poor',\n",
    "    'C4S2': 'Bad',\n",
    "    'C4S1': 'Bad',\n",
    "    'C3S2': 'Poor',\n",
    "    'C4S4': 'Bad',\n",
    "    'C4S3': 'Bad',\n",
    "    'C1S1': 'Good',\n",
    "    'C3S4': 'Bad',\n",
    "    'C3S3': 'Poor',\n",
    "    'C2S2': 'Moderate'\n",
    "}\n",
    "\n",
    "df_concat['Classification'] = df_concat['Classification'].replace(selection)\n",
    "df_concat = df_concat[df_concat['Classification'].isin(selection.values())]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changer les valeurs objet en format numerique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objet_columns = df_concat.select_dtypes(include='object').columns\n",
    "print(objet_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in objet_columns:\n",
    "    unique_values = df_concat[cols].unique()\n",
    "    print(cols,\" : \",unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le= LabelEncoder()\n",
    "\n",
    "for element in objet_columns:\n",
    "    df_concat[element]=le.fit_transform(df_concat[element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recuperation des 10 colone qui on le plus de corelation avec la target \"Classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_concat.corr()\n",
    "\n",
    "correlations_with_target = correlation_matrix['Classification'].abs()\n",
    "\n",
    "top_correlations = correlations_with_target.sort_values(ascending=False)\n",
    "\n",
    "top_features = top_correlations.head(11).index\n",
    "\n",
    "top_features_df = pd.DataFrame({'Top Features': top_features})\n",
    "\n",
    "clean_df = df_concat.loc[:, top_features]\n",
    "\n",
    "print(top_features_df)\n",
    "column_names = top_features_df['Top Features'].tolist()\n",
    "\n",
    "df_concat=df_concat.drop(columns=df_concat.columns.difference(column_names))\n",
    "df_concat.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ia stuff</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_concat.drop('Classification', axis=1)\n",
    "y=df_concat['Classification']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> model LeaveOneOut: </h2> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score  \n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "accuracy_scores = []\n",
    "loo = LeaveOneOut()\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = np.array(X.iloc[train_index]), np.array(X.iloc[test_index])\n",
    "    y_train, y_test = np.array(y.iloc[train_index]), np.array(y.iloc[test_index])\n",
    "\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred = knn_model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "average_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "print(\"Moyenne de précision avec LOO:\", average_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> model kFold:</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = np.array(X.iloc[train_index]), np.array(X.iloc[test_index])\n",
    "    y_train, y_test = np.array(y.iloc[train_index]), np.array(y.iloc[test_index])\n",
    "\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred = knn_model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "average_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "print(\"Moyenne de précision avec KFold:\", average_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>model train_test_split:</h2> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# X représente les caractéristiques et y les étiquettes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=5, weights=\"uniform\")\n",
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)\n",
    "neigh.fit(X_train, y_train)\n",
    "# neigh.predict(X_train)\n",
    "# neigh.predict_proba(X_train)\n",
    "neigh.score(X_test, y_test, sample_weight=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> model random forest:</h2> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grie d'evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, make_scorer\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 3],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted', zero_division=1),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1_score': make_scorer(f1_score, average='weighted')\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, scoring=scoring, cv=kf, refit='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "best_estimator = grid_search.best_estimator_\n",
    "best_estimator.fit(X, y)  \n",
    "y_pred = best_estimator.predict(X)  \n",
    "confusion = confusion_matrix(y, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "afichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*40)\n",
    "print(grid_search.best_estimator_.__class__.__name__)\n",
    "print(\"=\"*40)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"=\"*40)\n",
    "for metric_name, metric_scorer in scoring.items():\n",
    "    metric_value = grid_search.cv_results_[f\"mean_test_{metric_name}\"][grid_search.best_index_]\n",
    "    print(f\"mean {metric_name}: {metric_value:.4f}\")\n",
    "print(\"=\"*40)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"=\"*40)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, cmap='cool', fmt='d')\n",
    "plt.title(\"Matrice de confusion totale\")\n",
    "plt.xlabel(\"Classe prédite\")\n",
    "plt.ylabel(\"Classe réelle\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
